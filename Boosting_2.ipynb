{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is Gradient Boosting Regression?\n",
        "# Gradient Boosting Regression is an ensemble learning technique that builds a model by combining multiple weak learners, typically decision trees.\n",
        "# It works by training models sequentially, with each model focusing on the residuals (errors) of the previous models.\n",
        "# In each iteration, it tries to minimize the loss function (e.g., mean squared error) by fitting a new model that corrects the residuals of the previous models.\n",
        "\n",
        "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy.\n",
        "# Use a simple regression problem as an example and train the model on a small dataset.\n",
        "# Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a simple regression dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1)  # 100 data points, 1 feature\n",
        "y = 5 * X.squeeze() + np.random.randn(100) * 0.5  # Linear data with noise\n",
        "\n",
        "# Simple Gradient Boosting implementation (from scratch)\n",
        "class SimpleGradientBoosting:\n",
        "    def __init__(self, n_estimators=50, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []  # Store the weak learners (trees)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize the model with the mean of the target variable\n",
        "        y_pred = np.full_like(y, np.mean(y))\n",
        "        for i in range(self.n_estimators):\n",
        "            # Compute residuals\n",
        "            residuals = y - y_pred\n",
        "            # Fit a simple model (mean of residuals for simplicity, in practice we use decision trees)\n",
        "            model = np.mean(residuals)  # Weak learner (tree would normally be used here)\n",
        "            # Update the predictions\n",
        "            y_pred += self.learning_rate * model  # Gradient step\n",
        "            self.models.append(model)  # Save the model\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Start with initial prediction (mean of y)\n",
        "        y_pred = np.full_like(X, np.mean(y))\n",
        "        for model in self.models:\n",
        "            # Add the model's prediction to the overall prediction\n",
        "            y_pred += self.learning_rate * model  # In practice, we'd use the weak learner's prediction\n",
        "        return y_pred\n",
        "\n",
        "# Create and train the model\n",
        "model = SimpleGradientBoosting(n_estimators=50, learning_rate=0.1)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Evaluate performance\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth.\n",
        "# Grid search or random search can be used to find the best hyperparameters. Here, we can manually experiment with different settings.\n",
        "\n",
        "# For simplicity, let's experiment with different n_estimators and learning_rate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# In practice, we would use scikit-learn's GradientBoostingRegressor or use cross-validation for hyperparameter tuning.\n",
        "# Example: Grid Search for learning rate and number of estimators\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "# Define hyperparameters for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(gbr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(\"Best Hyperparameters from Grid Search:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the model with the best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "r2_best = r2_score(y_test, y_pred_best)\n",
        "print(\"Optimized Mean Squared Error:\", mse_best)\n",
        "print(\"Optimized R-squared:\", r2_best)\n",
        "\n",
        "# Q4. What is a weak learner in Gradient Boosting?\n",
        "# A weak learner in Gradient Boosting is typically a simple model, like a shallow decision tree (also known as a stump),\n",
        "# that performs slightly better than random guessing. These weak learners are iteratively trained and combined to form a stronger model.\n",
        "\n",
        "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "# Gradient Boosting works by fitting a sequence of models to the residuals (errors) of the previous models.\n",
        "# The idea is to correct the errors iteratively, with each model improving upon the predictions of the previous ones.\n",
        "# The model learns by reducing the loss function (e.g., MSE) using gradient descent, where the gradient of the loss guides the updates.\n",
        "\n",
        "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "# Gradient Boosting builds the ensemble by sequentially training weak learners (typically decision trees).\n",
        "# Each weak learner tries to correct the mistakes made by the previous learners.\n",
        "# The final prediction is the weighted sum of all the learners' predictions, where each learner is assigned a weight based on its performance.\n",
        "\n",
        "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
        "# The steps involved are:\n",
        "# 1. Initialize the model with a baseline prediction, usually the mean of the target variable.\n",
        "# 2. Calculate the residuals (errors) between the true values and the current predictions.\n",
        "# 3. Fit a weak learner (e.g., decision tree) to the residuals.\n",
        "# 4. Update the model's predictions by adding the weak learner's prediction, scaled by the learning rate.\n",
        "# 5. Repeat steps 2-4 for a specified number of iterations or until convergence.\n"
      ]
    }
  ]
}